{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZkfslX9rcsG4",
    "tags": []
   },
   "source": [
    "# Reinforcement Learning\n",
    "# Cours 2 : Programmation dynamique\n",
    "\n",
    "La programmation dynamique permet de résoudre des problèmes récursives en les décomposant en plus petits problèmes.\n",
    "Ce TP illustre plusieurs cas d'utilisations.\n",
    "\n",
    "\n",
    "1/4 de la note finale est liée à la mise en forme : \n",
    "\n",
    "* pensez à nettoyer les outputs inutiles (installation, messages de débuggage, ...)\n",
    "* soignez vos figures : les axes sont-ils faciles à comprendre ? L'échelle est adaptée ? \n",
    "* commentez vos résultats : vous attendiez-vous à les avoir ? Est-ce étonnant ? Faites le lien avec la théorie.\n",
    "\n",
    "Ce TP reprend l'exemple d'un médecin et de ses vaccins. Vous allez comparer plusieurs stratégies et trouver celle optimale.\n",
    "Un TP se fait en groupe de 2 à 4. Aucun groupe de plus de 4 personnes. \n",
    "\n",
    "Vous allez rendre le TP dans une archive ZIP. L'archive ZIP contient ce notebook au format `ipynb`, mais aussi exporté en PDF & HTML. \n",
    "L'archive ZIP doit aussi contenir un fichier txt appelé `groupe.txt` sous le format:\n",
    "\n",
    "```\n",
    "Nom1, Prenom1, Email1, NumEtudiant1\n",
    "Nom2, Prenom2, Email2, NumEtudiant2\n",
    "Nom3, Prenom3, Email3, NumEtudiant3\n",
    "Nom4, Prenom4, Email4, NumEtudiant4\n",
    "```\n",
    "\n",
    "Un script vient extraire vos réponses : ne changez pas l'ordre des cellules et soyez sûrs que les graphes sont bien présents dans la version notebook soumise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "E7iWrjHqcsHS"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tSssLu0pcsHT",
    "tags": []
   },
   "source": [
    "## I. Chaîne de Markov finie\n",
    "Un processus de décision markovien est basé sur **l'hypothèse de Markov**, qui stipule qu'en connaissant l'état à l'instant $t$, on sait tout ce qui s'est passé auparavant. \n",
    "\n",
    "Mathématiquement, cette hypothèse signifie que la probabilité d'une variable $X_{t+1}$ sachant l'état $ X_t $ est indépendante des états précédents :\n",
    "\n",
    "$$\n",
    "\\mathbb{P}(X_{t+1}| X_{t},X_{t-1},\\dots) = \\mathbb{P}(X_{t+1} | X_{t})\n",
    "$$\n",
    "\n",
    "Etudions dans un premier temps plusieurs exemples de chaîne de Markov."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RBBFCgWRcsHU"
   },
   "source": [
    "Ici, nous nous intéressons à la météo : $X_t$ représente le temps au jour $t$ et peut être ensoleillé, couvert ou pluvieux.\n",
    "\n",
    "![markov](img/markov3.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nc8Xab-RcsHV"
   },
   "source": [
    "**Q1. Ecrivez ci-dessous la valeur numérique de la matrice de transition avec PyTorch**\n",
    "\n",
    "$$\n",
    "P = \\left(\n",
    "\\begin{array}{lll}\n",
    "P_{11}, &\n",
    "P_{12}, &\n",
    "P_{13} \\\\\n",
    "P_{21}, &\n",
    "P_{22}, &\n",
    "P_{23} \\\\\n",
    "P_{31}, &\n",
    "P_{32}, &\n",
    "P_{33}\n",
    "\\end{array}\n",
    "\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gHWgXKhfcsHW",
    "outputId": "123415c5-71ae-4ccb-e170-a913866d4abf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5000, 0.3000, 0.2000],\n",
       "        [0.2000, 0.7000, 0.1000],\n",
       "        [0.6000, 0.3000, 0.1000]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "P = torch.tensor([[0.5,0.3,0.2], [0.2,0.7,0.1], [0.6,0.3,0.1]])\n",
    "\n",
    "P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TxUNfF2tcsHW"
   },
   "source": [
    "*(aucun commentaire n'est attendu pour cette question)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zWX12nxwcsHX"
   },
   "source": [
    "Une matrice est dite **stochastique** si elle représente une distribution de probabilité. Autrement dit, si ses lignes somment à 1 et que sont les éléments sont positifs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cFAXLm5ucsHX"
   },
   "source": [
    "**Q2. Vérifiez avec `torch` que votre matrice de transition est bien stochastique**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N0hcRt3RcsHY",
    "outputId": "b1196e4a-0f93-4eb5-95c9-6983c2c9ee45"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La matrice est stochastic\n"
     ]
    }
   ],
   "source": [
    "l_sum = [elem.sum() for elem in P]\n",
    "\n",
    "e_p = 1\n",
    "\n",
    "for i in (P > 0):\n",
    "  for j in i:\n",
    "    e_p *= j\n",
    "\n",
    "if ((l_sum == [1,1,1]) and e_p):\n",
    "  print(\"La matrice est stochastic\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "szcpkk4ocsHY"
   },
   "source": [
    "*[Ajoutez votre commentaire ici]*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fYASS4uEcsHY"
   },
   "source": [
    "On simule la météo sur 2 semaines. \n",
    "Pour cela, on tire une valeur aléatoire suivant la distribution, suivante, puis les états suivants sont obtenues grâce à un tirage aléatoire utilisant la matrice de transition précédente. \n",
    "\n",
    "$$\n",
    "\\mathbb{P}(X_0 = Soleil) = 0.2$$\n",
    "$$\\mathbb{P}(X_0 = Couvert) = 0.3$$\n",
    "$$\\mathbb{P}(X_0 = Pleuvieux) = 0.5$$\n",
    "\n",
    "\n",
    "**Q3. Simulez la chaîne de Markov sur 14 jours. Tracez la chaîne des états observée sur la forme d'un graph à l'aide de [`networkx`](https://stackoverflow.com/questions/51902156/plot-network-chain-in-python)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hide-output": false,
    "id": "LOUxr-WjcsHZ",
    "outputId": "ba0e4422-c98b-47ea-a9de-3846d867edc9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2000, 0.3000, 0.5000])\n",
      "tensor([[0.5000, 0.3000, 0.2000],\n",
      "        [0.2000, 0.7000, 0.1000],\n",
      "        [0.6000, 0.3000, 0.1000]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.3639, 0.4997, 0.1364])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X0 = torch.Tensor([0.2, 0.3, 0.5])\n",
    "\n",
    "states = ['soleil', 'couvert', 'pluvieux']\n",
    "\n",
    "P = torch.Tensor([[0.5, 0.3, 0.2],[0.2, 0.7, 0.1], [0.6, 0.3, 0.1]])\n",
    "\n",
    "print(X0)\n",
    "print(P)\n",
    "\n",
    "def get_state(days, X, P):\n",
    "    result = X\n",
    "    for i in range(days):\n",
    "        result = torch.matmul(result, P)\n",
    "    return result\n",
    "        \n",
    "get_state(7, X0, P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n5jMoDx6csHZ"
   },
   "source": [
    "*[Ajoutez votre commentaire ici]*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LHeMEkV3csHZ"
   },
   "source": [
    "La simulation repose sur un schéma itératif. On se demande désormais si on peut exprimer une *forme analytique* (ie. une expression littérale) pour calculer la probabilité d'un futur état.\n",
    "\n",
    "Commencons par chercher la distribution de $ X_{t+1} $ notée $ \\mathbb {P} \\{X_{t+1} \\}$\n",
    "\n",
    "**Q4. A l'aide des [théorèmes des probabilités](https://fr.wikipedia.org/wiki/Axiomes_des_probabilit%C3%A9s), démontrez le résultat suivant :**\n",
    "\n",
    "$$\n",
    "\\mathbb {P} \\{X_{t+1} = y \\}\n",
    "   = \\sum_{x \\in S} \\mathbb {P} \\{ X_{t+1} = y \\, | \\, X_t = x \\}\n",
    "               \\cdot \\mathbb {P} \\{ X_t = x \\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VLdmzMfIcsHa"
   },
   "source": [
    "D'après le théorème des probabilités cooditionnelles:  \n",
    " \n",
    " $$ {P} \\{ X_{t+1} = y \\, | \\, X_t = x \\} = \\frac{\\mathbb {P} \\{X_{t+1} = y  \\cup X_t = x \\}   }{\\mathbb {P} \\{ X_t = x \\}}  $$\n",
    " \n",
    " $$\n",
    "\\mathbb {P} \\{X_{t+1} = y \\cup X_t = x \\}\n",
    "   =  \\mathbb {P} \\{ X_{t+1} = y \\, | \\, X_t = x \\}\n",
    "               \\cdot \\mathbb {P} \\{ X_t = x \\}\n",
    "$$\n",
    "\n",
    "$$ \\mathbb \\sum_{x \\in S} {P} \\{X_{t+1} = y \\cup X_t = x \\}\n",
    "   = \\sum_{x \\in S} \\mathbb {P} \\{ X_{t+1} = y \\, | \\, X_t = x \\}\n",
    "               \\cdot \\mathbb {P} \\{ X_t = x \\}\n",
    "$$\n",
    "\n",
    "Or d'après la formule des porbabilités totales\n",
    "\n",
    "$$ \\mathbb  {P} \\{X_{t+1} = y \\}\n",
    "   = \\sum_{x \\in S} \\mathbb {P} \\{ X_{t+1} = y \\, | \\, X_t = x \\}\n",
    "               \\cdot \\mathbb {P} \\{ X_t = x \\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "szbe0Y2mcsHa"
   },
   "source": [
    "Autrement dit, pour connaître la probabilité d'avoir $y$ demain, il faut considérer tous les cas aujourd'hui !\n",
    "Avec la notation matricielle, cela s'écrit :\n",
    "\n",
    "$$\n",
    "\\mathbb {P} \\{X_{t+1} = x_j \\}\n",
    "   = \\sum_{x_i \\in S} \\mathbb {P} \\{ X_{t+1} = x_j | X_t = x_i \\}\n",
    "               \\cdot \\mathbb {P} \\{ X_t = x_i \\}\n",
    "   = \\sum_{i \\in \\{1,\\dots,n\\}} p_{ij} \\cdot \\psi_{t,i}\n",
    "   = \\pmb{\\Psi}_{t} \\cdot \\pmb{P}\n",
    "$$\n",
    "\n",
    "avec $ \\psi_t $ , la distribution de probabilité à l'instant $t$. \n",
    "\n",
    "En appliquant cette formule récursivement, on obtient la probabilité de la météo au N-ème jour à partir d'un simple calcul matriciel :\n",
    "\n",
    "$$\n",
    "\\pmb{\\Psi}_{t+N} = \\pmb{\\Psi}_{t+N-1} \\cdot \\pmb{P} = \\cdots = \\pmb{\\Psi}_{t} \\cdot \\pmb{P}^N\n",
    "$$\n",
    "\n",
    "**Q5. Faire l'application numérique pour le 14ème jour ($\\pmb{\\Psi}_0$ est donné plus haut).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v17YlAIccsHb",
    "outputId": "4fc5a1ea-81cd-4dda-ddad-48d1860d693b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3636, 0.5000, 0.1364])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Phi0 = torch.tensor([0.2, 0.3, 0.5]) \n",
    "\n",
    "Phi = Phi0 @ torch.matrix_power(P, 14) \n",
    "\n",
    "Phi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "21k04kTmcsHb"
   },
   "source": [
    "*[Ajoutez votre commentaire ici]*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ML4xzttTcsHb"
   },
   "source": [
    "## II. Convergence vers un état stationnaire d'une chaîne de Markov.\n",
    "\n",
    "Cette partie illustre la convergence d'une chaîne de Markov vers une **distribution stationnaire**. \n",
    "\n",
    "Une distribution $ \\psi^\\star $  est dite **stationnaire** ou **invariante** si $$\\pmb{\\Psi}^\\star = \\pmb{\\Psi}^\\star \\cdot \\pmb{P}$$\n",
    "\n",
    "Ainsi, $$\\pmb{\\Psi}^\\star = \\pmb{\\Psi}^\\star \\cdot\\pmb{P}^k \\text{ pour tout } k.$$ \n",
    "\n",
    "De plus, si $ X_0 $ a la distribution $ \\pmb{\\Psi}^\\star $, alors $ X_t $ aura la même distribution pour tout $ t $.\n",
    "\n",
    "***Théorème** : toute matrice stochastique $\\pmb{P}$ admet au moins une distribution stationnaire $\\pmb{\\Psi}$*\n",
    "\n",
    "**Q6. Trouvez la distribution stationnaire pour le champ de Markov précédemment défini.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GWZzCKWucsHb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CC-P4YmwcsHb"
   },
   "source": [
    "*[Ajoutez votre commentaire ici]*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ZjFp65acsHb"
   },
   "source": [
    "*Théorème : si une matrice stochastique est apériodique (aucune répétition est prévisible dans la chaîne de Markov) et irréductible (on peut passer d'un état à un autre en un temps fini), alors:*\n",
    "\n",
    "1. *la matrice stochastique admet une unique distribution stationnaire $ \\psi^\\star $*\n",
    "1. *la chaîne de Markov converge vers cette distribution pour toute distribution initiale $ \\psi_0 $ : $ \\| \\psi_0 P^t - \\psi^\\star \\| \\to 0 $ as $ t \\to \\infty $*\n",
    "\n",
    "Une matrice satisfaisant ces conditions est dite **uniformément ergodique**.\n",
    "Une condition suffisante pour cela est que tous les éléments de cette matrice \\$ P \\$ sont strictement positifs.\n",
    "\n",
    "**Q7. Calculez plusieurs fonctions 3x3 aléatoires uniformément ergodiques et estimez leur distribution stationnaire.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false,
    "id": "v53jMvo5csHe"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "013-M-R6csHf"
   },
   "source": [
    "*[Ajoutez votre commentaire ici]*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sMlHeClDcsHf"
   },
   "source": [
    "Ces propriétés sont fondamentales : nous allons les utiliser pour estimer la fonction de valeur optimale !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cb6ut54tcsHf"
   },
   "source": [
    "## III. La programmation dynamique dans des environnements discrets \n",
    "\n",
    "L'idée clé de la programmation dynamique de casser la résolution de problèmes complexes en les décomposant pour en sous-problème, passant d'un temps de résolution exponentielle à un temps polynomial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V4mWjLLPcsHg"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RZsFcNTRcsHg"
   },
   "source": [
    "*[Ajoutez votre commentaire ici]*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RkkoR3gBcsHg"
   },
   "source": [
    "### III. 1. Estimation de la fonction de valeur d'un gridword\n",
    "\n",
    "Nous avons vu en cours que :\n",
    "\n",
    "$$v_\\pi (s) = \\mathbb{E}_\\pi \\left( G_t | s \\right) = \\sum_{s'} p(s'|s, a)\\left[r+\\gamma v_\\pi(s') \\right]$$\n",
    "\n",
    "Dans le cas où les dynamiques de l'environnement sont entièrement connus, $p(s'|s, a)$ peut s'exprimer sous la forme d'un tensor et l'équation précédente aboutit à un système d'équations linéaires. Le problème est donc résolvable, mais la résolution risque d'être longue si l'environnement est grand. \n",
    "\n",
    "On cherche plutôt une résolution itérative en appliquant les principes de la programmation dynamique. Concrètement, on part d'une fonction de valeur arbitraire $v_0$ (par exemple nulle partout), puis on y applique à chaque étape l'équation de Bellman :\n",
    "$$v_{k+1} (s) = \\sum_{s'} p(s'|s, a)\\left[r+\\gamma v_k(s') \\right]$$\n",
    "Lorsque l'algorithme a convergé vers un point fixe $v_\\infty$, nous avons fini d'évaluer $v_\\pi$, puisque ce dernier est l'unique point fixe de la fonction de valeur.\n",
    "\n",
    "Cet algorithme est appelé l'**évaluation itérative de la politique**.\n",
    "\n",
    "On considère par la suite le \"gridworld\" suivant :\n",
    "\n",
    "![gridworld](img/grid-world.png)\n",
    "\n",
    "Les cases grisées sont terminales et la récompense est de -1 sur toutes les transitions.\n",
    "La taille du gridworld est une constante `CUBE_SIDE`.\n",
    "\n",
    "**Q8: évaluez la fonction de valeur de la politique aléatoire à l'aide d'un algorithme itératif. Arrếtez l'algorithme lorsque les valeurs n'ont pas évolué de plus de 1e-2.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-pEWtpY8csHg"
   },
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "import typing as t\n",
    "from dataclasses import dataclass, field\n",
    "import random\n",
    "import torch\n",
    "\n",
    "Action = t.Literal[\"L\", \"R\", \"U\" , \"D\"]\n",
    "CUBE_SIDE = 6\n",
    "\n",
    "@dataclass\n",
    "class State: \n",
    "    \"\"\"\n",
    "    It represents any cell in the world\n",
    "    \"\"\"\n",
    "    cell: int\n",
    "    value: int = 0\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        self.bounds = {\n",
    "            'L': self.cell - self.cell % CUBE_SIDE,\n",
    "            'R': self.cell - self.cell % CUBE_SIDE + (CUBE_SIDE - 1),\n",
    "            'U': self.cell % CUBE_SIDE,\n",
    "            'D': self.cell % CUBE_SIDE + CUBE_SIDE * (CUBE_SIDE - 1),\n",
    "        }\n",
    "        self.neighbors = [self.act(a) for a in \"LRUD\"]\n",
    "        assert all(i >= 0 and i < CUBE_SIDE*CUBE_SIDE for i in self.neighbors)\n",
    "    \n",
    "    def is_termination(self):\n",
    "        return self.cell in {0, CUBE_SIDE * CUBE_SIDE - 1}\n",
    "\n",
    "    def act(self, a: Action):\n",
    "        \"\"\"\n",
    "        Get next state\n",
    "        \"\"\"\n",
    "        if a == 'L': \n",
    "            return min(self.bounds['R'], max(self.bounds['L'], self.cell - 1))\n",
    "        if a == 'R': \n",
    "            return min(self.bounds['R'], max(self.bounds['L'], self.cell + 1))\n",
    "        if a == 'U': \n",
    "            return min(self.bounds['D'], max(self.bounds['U'], self.cell - 4))\n",
    "        if a == 'D':\n",
    "            return min(self.bounds['D'], max(self.bounds['U'], self.cell + 4))\n",
    "        raise ValueError('Unexpected action')\n",
    "    \n",
    "\n",
    "def init_states():\n",
    "    return [State(i) for i in range(CUBE_SIDE * CUBE_SIDE)]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Env:\n",
    "    states: t.List[State] = field(default_factory=init_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dwU5ZcQmcsHh"
   },
   "source": [
    "La politique gloutonne cherche uniquement à exploiter, sans aucune exploration. A chaque instant, elle choisit l'action qui permet de maximiser la fonction de valeur :\n",
    "\n",
    "$$\\pi(s) = \\text{argmax}_a \\sum_{s'} p(s'|s,a)[r+\\gamma V(s')]$$\n",
    "\n",
    "**Q9: calculez la politique ainsi obtenue. Vérifiez qu'il s'agit de la politique optimale. Combien d'étapes ont été nécessaires pour obtenir ce résultat ?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XEtAv_b8csHh"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G2kR7Ly0csHh"
   },
   "source": [
    "*[Ajoutez votre commentaire ici]*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9u0n9Hg8csHh"
   },
   "source": [
    "### III. 2. Algorithme *policy iteration*\n",
    "\n",
    "Une amélioration de l'algorithme consiste 1) à évaluer la fonction de valeur sur un petit nombre d'itérations (on testera en Q10 avec une seule itération), puis 2) à mettre à jour la politique, puis à recommencer l'étape 1). On peut arrếter l'entraînement lorsque la politique a convergé.\n",
    "\n",
    "**Q10: implémentez cet algorithme. Est-il plus rapide ?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IiRtfhxKcsHi"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ML0M1t3kcsHi"
   },
   "source": [
    "*[Ajoutez votre commentaire ici]*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6cypko04csHl"
   },
   "source": [
    "### III. 4. Algorithme *value iteration*\n",
    "\n",
    "Une autre variante conserve la politique aléatoire tout en long de l'entraînement, mais met à jour la fonction de valeur avec l'équation suivante :\n",
    "\n",
    "$$v_{k+1} (s) = \\max_{a} \\sum_{s'} p(s'|s, a)\\left[r+\\gamma v_k(s') \\right]$$\n",
    "\n",
    "Une fois que la fonction de valeur a convergé, on calcule la politique avec :\n",
    "\n",
    "$$\\pi(s) = argmax_a \\sum_{s'} p(s'|s,a)[r+\\gamma V(s')]$$\n",
    "\n",
    "\n",
    "**Q11: implémentez cet algorithme. Quel algorithme vous paraît le plus judicieux ?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sffbpqqPcsHl"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4-bsoXzwcsHl"
   },
   "source": [
    "*[Ajoutez votre commentaire ici]*"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "provenance": []
  },
  "date": 1612589585.625011,
  "download_nb": false,
  "filename": "20_markov.rst",
  "filename_with_path": "20_markov",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "title": "Foundations of Computational Economics #20"
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
