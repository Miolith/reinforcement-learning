{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "6kHY6AnL2Mfj"
      },
      "source": [
        "# Reinforcement Learning\n",
        "# Cours 3 : Policy Iteration and Value Iteration\n",
        "\n",
        "Pour trouver une politique optimale, il existe deux grandes familles d'algorithmes : la programmation dynamique (résoudre le problème en le décomposant récursivement en plus petits problèmes) et les simulations de Monte-Carlo (faire des expériences pour estimer les distributions de probabilités). \n",
        "\n",
        "Dans ce TP, nous étudions deux types d'algorithme utilisant la programmation dynamique : les itérations sur les valeurs et les itérations sur la politique.\n",
        "\n",
        "\n",
        "RAPPEL : 1/4 de la note finale est liée à la mise en forme : \n",
        "\n",
        "* pensez à nettoyer les outputs inutiles (installation, messages de débuggage, ...)\n",
        "* soignez vos figures : les axes sont-ils faciles à comprendre ? L'échelle est adaptée ? \n",
        "* commentez vos résultats : vous attendiez-vous à les avoir ? Est-ce étonnant ? Faites le lien avec la théorie.\n",
        "\n",
        "Ce TP reprend l'exemple d'un médecin et de ses vaccins. Vous allez comparer plusieurs stratégies et trouver celle optimale.\n",
        "Un TP se fait en groupe de 2 à 4. Aucun groupe de plus de 4 personnes. \n",
        "\n",
        "Vous allez rendre le TP dans une archive ZIP. L'archive ZIP contient ce notebook au format `ipynb`, mais aussi exporté en PDF & HTML. \n",
        "L'archive ZIP doit aussi contenir un fichier txt appelé `groupe.txt` sous le format:\n",
        "\n",
        "```\n",
        "Nom1, Prenom1, Email1, NumEtudiant1\n",
        "Nom2, Prenom2, Email2, NumEtudiant2\n",
        "Nom3, Prenom3, Email3, NumEtudiant3\n",
        "Nom4, Prenom4, Email4, NumEtudiant4\n",
        "```\n",
        "\n",
        "Un script vient extraire vos réponses : ne changez pas l'ordre des cellules et soyez sûrs que les graphes sont bien présents dans la version notebook soumise. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "O-1fDfXV2Mfl"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import networkx as nx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTj6K6i_2Mfm"
      },
      "source": [
        "### I. Estimation de la fonction de valeur d'un gridword\n",
        "\n",
        "Nous avons vu en cours que :\n",
        "\n",
        "$$v_\\pi (s) = \\mathbb{E}_\\pi \\left( G_t | s \\right) = \\sum_{s'} p(s'|s, a)\\left[r+\\gamma v_\\pi(s') \\right]$$\n",
        "\n",
        "Dans le cas où les dynamiques de l'environnement sont entièrement connus, $p(s'|s, a)$ peut s'exprimer sous la forme d'un tensor et l'équation précédente aboutit à un système d'équations linéaires. Le problème est donc résolvable, mais la résolution risque d'être longue si l'environnement est grand. \n",
        "\n",
        "On cherche plutôt une résolution itérative qui applique le principe de la programmation dynamique. Concrètement, on part d'une fonction de valeur arbitraire $v_0$ (par exemple nulle partout), puis on y applique à chaque étape l'équation de Bellman :\n",
        "$$v_{k+1} (s) = \\sum_{s'} p(s'|s, a)\\left[r+\\gamma v_k(s') \\right]$$\n",
        "Lorsque l'algorithme a convergé vers un point fixe $v_\\infty$, nous avons fini d'évaluer $v_\\pi$, puisque ce dernier est l'unique point fixe de la fonction de valeur.\n",
        "\n",
        "Cet algorithme est appelé l'**évaluation itérative de la politique**.\n",
        "\n",
        "On considère par la suite le \"gridworld\" suivant :\n",
        "\n",
        "![gridworld](https://github.com/Miolith/reinforcement-learning/blob/master/img/grid-world.png?raw=1)\n",
        "\n",
        "Les cases grisées sont terminales et la récompense est de -1 sur toutes les transitions.\n",
        "La taille du gridworld est une constante `CUBE_SIDE`.\n",
        "\n",
        "**Q1: évaluez la fonction de valeur de la politique aléatoire à l'aide d'un algorithme itératif. Arrếtez l'algorithme lorsque les valeurs n'ont pas évolué de plus de 1e-2.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "hgbSbXMc2Mfm",
        "outputId": "6aaddbb5-4d94-4e75-915a-090a933d3f77"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0, 0),\n",
              " (1, -6.644668970140989),\n",
              " (2, -8.68225421516144),\n",
              " (3, -9.413514717900709),\n",
              " (4, -9.69125498430674),\n",
              " (5, -9.782938219363281),\n",
              " (6, -9.427027105147564),\n",
              " (7, -9.630849500978723),\n",
              " (8, -9.7646744814652),\n",
              " (9, -9.802493129133484),\n",
              " (10, -9.735782202695594),\n",
              " (11, -9.768537554673816),\n",
              " (12, -9.844025399963348),\n",
              " (13, -9.852570127811475),\n",
              " (14, -9.841079067567392),\n",
              " (15, -9.849345780530232),\n",
              " (16, -9.865711368173262),\n",
              " (17, -9.869882137893072),\n",
              " (18, -9.86875884453481),\n",
              " (19, -9.867403191623888),\n",
              " (20, -9.851942456141636),\n",
              " (21, -9.845424211062365),\n",
              " (22, -9.857624385532201),\n",
              " (23, -9.852112821192398),\n",
              " (24, -9.774854160107823),\n",
              " (25, -9.744663931849026),\n",
              " (26, -9.812051103588985),\n",
              " (27, -9.776468702472812),\n",
              " (28, -9.643972658952798),\n",
              " (29, -9.44308988082211),\n",
              " (30, -9.797018419317242),\n",
              " (31, -9.708595922048733),\n",
              " (32, -9.43263453670931),\n",
              " (33, -8.70188513322465),\n",
              " (34, -6.660086319100251),\n",
              " (35, 0)]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "from tabulate import tabulate\n",
        "import typing as t\n",
        "from dataclasses import dataclass, field\n",
        "import random\n",
        "import torch\n",
        "from time import time\n",
        "\n",
        "Action = t.Literal[\"L\", \"R\", \"U\" , \"D\"]\n",
        "CUBE_SIDE = 6\n",
        "\n",
        "@dataclass\n",
        "class State: \n",
        "    \"\"\"\n",
        "    It represents any cell in the world\n",
        "    \"\"\"\n",
        "    cell: int\n",
        "    value: int = 0\n",
        "    \n",
        "    def __post_init__(self):\n",
        "        self.bounds = {\n",
        "            'L': self.cell - self.cell % CUBE_SIDE,\n",
        "            'R': self.cell - self.cell % CUBE_SIDE + (CUBE_SIDE - 1),\n",
        "            'U': self.cell % CUBE_SIDE,\n",
        "            'D': self.cell % CUBE_SIDE + CUBE_SIDE * (CUBE_SIDE - 1),\n",
        "        }\n",
        "        self.neighbors = [self.act(a) for a in \"LRUD\"]\n",
        "        assert all(i >= 0 and i < CUBE_SIDE*CUBE_SIDE for i in self.neighbors)\n",
        "    \n",
        "    def is_termination(self):\n",
        "        return self.cell in {0, CUBE_SIDE * CUBE_SIDE - 1}\n",
        "\n",
        "    def act(self, a: Action):\n",
        "        \"\"\"\n",
        "        Get next state\n",
        "        \"\"\"\n",
        "        if a == 'L': \n",
        "            return min(self.bounds['R'], max(self.bounds['L'], self.cell - 1))\n",
        "        if a == 'R': \n",
        "            return min(self.bounds['R'], max(self.bounds['L'], self.cell + 1))\n",
        "        if a == 'U': \n",
        "            return min(self.bounds['D'], max(self.bounds['U'], self.cell - 4))\n",
        "        if a == 'D':\n",
        "            return min(self.bounds['D'], max(self.bounds['U'], self.cell + 4))\n",
        "        raise ValueError('Unexpected action')\n",
        "    \n",
        "\n",
        "def init_states():\n",
        "    return [State(i) for i in range(CUBE_SIDE * CUBE_SIDE)]\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class Env:\n",
        "    states: t.List[State] = field(default_factory=init_states)\n",
        "\n",
        "def random_policy_value(env: Env, gamma: float = 0.9, epsilon: float = 1e-2):\n",
        "    \"\"\"\n",
        "        Compute the values of the random policy\n",
        "    \"\"\"\n",
        "    states = env.states\n",
        "    num_iterations = 0\n",
        "    while True:\n",
        "        num_iterations += 1\n",
        "        delta = 0\n",
        "        for s in states:\n",
        "            if s.is_termination():\n",
        "                continue\n",
        "            v = s.value\n",
        "            s.value = sum([\n",
        "                    0.25 * (-1 + gamma * states[s.act(a)].value)\n",
        "                for a in \"LRUD\"])\n",
        "            delta = max(delta, abs(v - s.value))\n",
        "        if delta < epsilon:\n",
        "            break\n",
        "    return num_iterations, env\n",
        "\n",
        "env = Env()\n",
        "num_iterations, env = random_policy_value(env)\n",
        "\n",
        "[(s.cell, s.value) for s in env.states]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmP1RrS42Mfn"
      },
      "source": [
        "La politique gloutonne cherche uniquement à exploiter, sans aucune exploration. A chaque instant, elle choisit l'action qui permet de maximiser la fonction de valeur :\n",
        "\n",
        "$$\\pi(s) = \\text{argmax}_a \\sum_{s'} p(s'|s,a)[r+\\gamma V(s')]$$\n",
        "\n",
        "**Q2: calculez la politique ainsi obtenue. Vérifiez qu'il s'agit de la politique optimale. Combien d'itérations ont été nécessaires pour obtenir ce résultat ?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "ALJpH5H92Mfo",
        "outputId": "7a26dad1-a687-4c76-cbdb-dddf0569887e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nombre d'itérations : 31\n"
          ]
        }
      ],
      "source": [
        "def get_policy(env: Env, gamma: float = 0.9):\n",
        "    \"\"\"\n",
        "    Compute the optimal policy\n",
        "    \"\"\"\n",
        "    states = env.states\n",
        "    policy = {}\n",
        "    for s in states:\n",
        "        if s.is_termination():\n",
        "            continue\n",
        "        policy[s.cell] = max([\n",
        "            (a, 0.25 * (-1 + gamma * states[s.act(a)].value))\n",
        "            for a in \"LRUD\"\n",
        "        ], key=lambda x: x[1])[0]\n",
        "    return policy\n",
        "\n",
        "get_policy(env)\n",
        "\n",
        "print(\"nombre d'itérations :\", num_iterations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piLOjVLN2Mfo"
      },
      "source": [
        "*[Ajoutez votre commentaire ici]*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQqEMbyw2Mfp"
      },
      "source": [
        "### II. Algorithme *policy iteration*\n",
        "\n",
        "Une amélioration de l'algorithme consiste 1) à évaluer la fonction de valeur sur un petit nombre d'itérations (on testera en Q3 avec une seule itération), puis 2) à mettre à jour la politique, puis à recommencer l'étape 1). On peut arrếter l'entraînement lorsque la politique a convergé.\n",
        "\n",
        "**Q3: implémentez cet algorithme. Est-il plus rapide ?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "32p_i9uh2Mfp",
        "outputId": "f6a07601-103b-4099-f676-d3b48457f598"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "policy_iteration prend 3.5028 ms\n",
            "random_policy_value prend 4.9827 ms\n"
          ]
        }
      ],
      "source": [
        "def policy_iteration(env: Env, n: int, gamma: float = 0.9, epsilon: float = 1e-2):\n",
        "    \"\"\"\n",
        "      Value iteration algorithm with n iteration\n",
        "    \"\"\"\n",
        "    states = env.states\n",
        "    p = {}\n",
        "    while True:\n",
        "        for i in range(n):\n",
        "            for s in states:\n",
        "                if s.is_termination():\n",
        "                    continue\n",
        "                v = s.value\n",
        "                s.value = sum([\n",
        "                        0.25 * (-1 + gamma * states[s.act(a)].value)\n",
        "                    for a in \"LRUD\"])\n",
        "        p_k = p\n",
        "        p = get_policy(env, gamma)\n",
        "        if p_k == p:\n",
        "            break\n",
        "        \n",
        "    return p\n",
        "\n",
        "env = Env()\n",
        "\n",
        "start = time()\n",
        "policy_iteration(env, n=1)\n",
        "end = time()\n",
        "\n",
        "print(\"policy_iteration prend %.4f ms\" % ((end - start) * 1000))\n",
        "\n",
        "env = Env()\n",
        "\n",
        "start = time()\n",
        "V = random_policy_value(env)\n",
        "get_policy(env)\n",
        "end = time()\n",
        "\n",
        "print(\"random_policy_value prend %.4f ms\" % ((end - start) * 1000))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2B8VdK62Mfp"
      },
      "source": [
        "*Policy Iteration* est fois plus rapide que l'implémentation précédente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4g98PvS2Mfp"
      },
      "source": [
        "### III. Algorithme *value iteration*\n",
        "\n",
        "Une autre variante conserve la politique optimale tout en long de l'entraînement, mais met à jour la fonction de valeur avec l'équation suivante :\n",
        "\n",
        "$$v_{k+1} (s) = \\max_{a} \\sum_{s'} p(s'|s, a)\\left[r+\\gamma v_k(s') \\right]$$\n",
        "\n",
        "Une fois que la fonction de valeur a convergé, on calcule la politique avec :\n",
        "\n",
        "$$\\pi(s) = argmax_a \\sum_{s'} p(s'|s,a)[r+\\gamma V(s')]$$\n",
        "\n",
        "\n",
        "**Q4: implémentez cet algorithme.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "z7lo23x02Mfp",
        "outputId": "9ea2293c-31cc-464f-ea66-b3df7d365028"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "value_iteration prend 1.5264 ms\n"
          ]
        }
      ],
      "source": [
        "def value_iteration(env: Env, gamma: float = 0.9, epsilon: float = 1e-2):\n",
        "    \"\"\"\n",
        "        Value iteration algorithm\n",
        "    \"\"\"\n",
        "    states = env.states\n",
        "    while True:\n",
        "        delta = 0\n",
        "        for s in states:\n",
        "            if s.is_termination():\n",
        "                continue\n",
        "            v = s.value\n",
        "            s.value = max([\n",
        "                    0.25 * (-1 + gamma * states[s.act(a)].value)\n",
        "                for a in \"LRUD\"])\n",
        "            delta = max(delta, abs(v - s.value))\n",
        "        if delta < epsilon:\n",
        "            break\n",
        "    return env\n",
        "\n",
        "env = Env()\n",
        "\n",
        "start = time()\n",
        "V = value_iteration(env)\n",
        "p = get_policy(env)\n",
        "end = time()\n",
        "\n",
        "print(\"value_iteration prend %.4f ms\" % ((end - start) * 1000))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69wlwTM42Mfp"
      },
      "source": [
        "*[Ajoutez votre commentaire ici]*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gz22jlIr2Mfq"
      },
      "source": [
        "**Q5: Quel algorithme vous paraît le plus judicieux ?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0bU5gEl2Mfq"
      },
      "source": [
        "L'algorithme *Value iteration* est le plus judicieux car en étant le plus rapide, il obtient un résultat tout autant optimal."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nP2rX2PENTFE"
      },
      "execution_count": 5,
      "outputs": []
    }
  ],
  "metadata": {
    "celltoolbar": "Slideshow",
    "date": 1612589585.625011,
    "download_nb": false,
    "filename": "20_markov.rst",
    "filename_with_path": "20_markov",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0rc1"
    },
    "title": "Foundations of Computational Economics #20",
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}