{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Miolith/reinforcement-learning/blob/master/tp5_dqn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51d3490c-319e-4de2-b16f-3765fa2ec696",
      "metadata": {
        "id": "51d3490c-319e-4de2-b16f-3765fa2ec696"
      },
      "source": [
        "# Introduction au DQN avec PyTorch et gym\n",
        "\n",
        "Dans ce TP, vous allez apprendre à créer un réseau de neurones profond d'estimation de la qualité (DQN) pour résoudre des problèmes d'apprentissage par renforcement en utilisant PyTorch et gym.\n",
        "\n",
        "Pour ce faire, vous allez suivre les étapes suivantes:\n",
        "\n",
        "-    Installer PyTorch et gym sur votre ordinateur\n",
        "-    Définir un modèle DQN simple en utilisant PyTorch\n",
        "-    Entraîner le modèle DQN en itérant sur des épisodes de l'environnement\n",
        "-    Expérimenter avec différents environnements, hyperparamètres et architectures pour voir comment ils affectent la performance du DQN\n",
        "\n",
        "Avant de commencer, assurez-vous d'avoir installé Python 3 et les bibliothèques nécessaires sur votre ordinateur. Si vous avez des \n",
        "questions, n'hésitez pas à demander de l'aide à votre enseignant ou à vos camarades. Bon travail!\n",
        "\n",
        "#### Rappels\n",
        "\n",
        "Le DQN apprend à jouer à un jeu en interagissant avec l'environnement et en essayant de maximiser la récompense future. Il fait cela en utilisant un réseau de neurones pour estimer la fonction de valeur, qui est une estimation de la somme des récompenses futures attendues à partir de l'état et de l'action actuels. Le DQN apprend à jouer en prenant des actions qui maximisent la fonction de valeur estimée et en ajustant ses paramètres en fonction de la différence entre la fonction de valeur estimée et la cible de la fonction de valeur.\n",
        "\n",
        "#### Objectifs\n",
        "\n",
        "-    Apprendre les concepts de base de l'apprentissage par renforcement et de l'estimation de la qualité\n",
        "-    Installer et utiliser PyTorch et gym pour créer des modèles DQN\n",
        "-    Comprendre les différents éléments d'un DQN, tels que les fonctions de perte et d'optimisation\n",
        "-    Expérimenter avec différents environnements, hyperparamètres et architectures pour améliorer la performance du DQN\n",
        "\n",
        "En réalisant ce TP, les étudiants devraient être en mesure de créer des modèles DQN simples et d'utiliser les outils PyTorch et gym pour les entraîner et les tester. Ils devraient également comprendre comment différents éléments d'un DQN peuvent affecter son apprentissage et sa performance.\n",
        "\n",
        "#### Ressources\n",
        "\n",
        "Voici une liste de ressources que vous pouvez utiliser en complément du cours pour en savoir plus sur les DQN, PyTorch et l'apprentissage par renforcement:\n",
        "\n",
        "-    Vidéos sur YouTube:\n",
        "        - [Introduction to Deep Q-Networks (DQN) with PyTorch](https://www.youtube.com/watch?v=MtMeio2R0Dk)\n",
        "        - [PyTorch Tutorial for Beginners (Full Course)](https://www.youtube.com/watch?v=VZYc_ZHrruE)\n",
        "        - [Reinforcement Learning Basics with OpenAI Gym](https://www.youtube.com/watch?v=Q-BpqyOT3a8)\n",
        "-    Livres:\n",
        "        - [Deep Learning with PyTorch](https://www.amazon.com/Deep-Learning-PyTorch-Vishnu-Subramanian/dp/1492045527)\n",
        "        - [Reinforcement Learning: An Introduction](https://www.amazon.com/Reinforcement-Learning-Introduction-Richard-Sutton/dp/0262193981)\n",
        "        - [Hands-On Reinforcement Learning with PyTorch](https://www.amazon.com/Hands-Reinforcement-Learning-PyTorch-Approach/dp/1838649322)\n",
        "\n",
        "Ces ressources vous permettront de vous familiariser avec les concepts de base des DQN, de PyTorch et de l'apprentissage par renforcement, et de vous fournir des exemples et des astuces pour utiliser ces outils dans vos propres projets. Vous pouvez également consulter d'autres ressources en ligne, telles que des tutoriels, des forums et des documentations pour en apprendre davantage.\n",
        "\n",
        "#### RAPPEL : 1/4 de la note finale est liée à la mise en forme : \n",
        "\n",
        "- Pensez à nettoyer les outputs inutiles (installation, messages de débuggage, ...)\n",
        "- Soignez vos figures : les axes sont-ils faciles à comprendre ? L'échelle est adaptée ? \n",
        "- Commentez vos résultats : vous attendiez-vous à les avoir ? Est-ce étonnant ? Faites le lien avec la théorie.\n",
        "\n",
        "Ce TP reprend l'exemple d'un médecin et de ses vaccins. Vous allez comparer plusieurs stratégies et trouver celle optimale.\n",
        "Un TP se fait en groupe de 2 à 4. Aucun groupe de plus de 4 personnes. \n",
        "\n",
        "Vous allez rendre le TP dans une archive ZIP. L'archive ZIP contient ce notebook au format `ipynb`, mais aussi exporté en PDF & HTML. \n",
        "L'archive ZIP doit aussi contenir un fichier txt appelé `groupe.txt` sous le format:\n",
        "\n",
        "```\n",
        "Nom1, Prenom1, Email1, NumEtudiant1\n",
        "Nom2, Prenom2, Email2, NumEtudiant2\n",
        "Nom3, Prenom3, Email3, NumEtudiant3\n",
        "Nom4, Prenom4, Email4, NumEtudiant4\n",
        "```\n",
        "\n",
        "Un script vient extraire vos réponses : ne changez pas l'ordre des cellules et soyez sûrs que les graphes sont bien présents dans la version notebook soumise. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30541ebd-b84f-4789-b6fd-49ff2cd0bfe6",
      "metadata": {
        "id": "30541ebd-b84f-4789-b6fd-49ff2cd0bfe6"
      },
      "source": [
        "**Q1: Quelle est la différence entre un réseau neuronal profond (DQN) et un réseau neuronal classique ?**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "559ace1a-aa44-47fc-ab83-51e732c000da",
      "metadata": {
        "id": "559ace1a-aa44-47fc-ab83-51e732c000da"
      },
      "source": [
        "*Un DNN est un réseau neuronal avec de nombreuses couches cachées alors qu'un DQN est un type spécifique de DNN utilisé dans l'apprentissage par renforcement pour prendre des décisions en maximisant une récompense.*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86dee66b-f808-40db-bff9-71a814e8f2d5",
      "metadata": {
        "id": "86dee66b-f808-40db-bff9-71a814e8f2d5"
      },
      "source": [
        "**Q2. Dans quel type de problème est utilisé le DQN ?**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8faaea54-b02d-4558-9843-8a8fe24dc266",
      "metadata": {
        "id": "8faaea54-b02d-4558-9843-8a8fe24dc266"
      },
      "source": [
        "*les DQN sont utilisés dans des tâches où il est difficile de définir explicitement les règles à suivre pour atteindre un objectif, mais où il est possible de mesurer la qualité des décisions prises par l'agent en mettant en place un système de récompense.*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2570acc6-2da7-4881-b754-449694fe165c",
      "metadata": {
        "id": "2570acc6-2da7-4881-b754-449694fe165c"
      },
      "source": [
        "**Q3. Quelle est la différence entre l'espace des actions et des états dans l'environnement [`mountaincar-v0`](https://gym.openai.com/envs/MountainCar-v0/) et `cartpole-v0` ? Comment pouvez-vous visualiser cet environnement ?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "92e0bf7d-bad2-45e3-a3fc-a2a685399a71",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 535
        },
        "id": "92e0bf7d-bad2-45e3-a3fc-a2a685399a71",
        "outputId": "47dd6a52-6559-4d8f-d510-ace51a88bd56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.8/dist-packages (0.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.8/dist-packages (from gym) (0.0.8)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.8/dist-packages (from gym) (5.2.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.8/dist-packages (from gym) (1.21.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.8.0->gym) (3.11.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[classic_control] in /usr/local/lib/python3.8/dist-packages (0.25.2)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.8/dist-packages (from gym[classic_control]) (0.0.8)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from gym[classic_control]) (1.5.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.8/dist-packages (from gym[classic_control]) (5.2.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.8/dist-packages (from gym[classic_control]) (1.21.6)\n",
            "Collecting pygame==2.1.0\n",
            "  Downloading pygame-2.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m73.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.8.0->gym[classic_control]) (3.11.0)\n",
            "Installing collected packages: pygame\n",
            "Successfully installed pygame-2.1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/gym/envs/registration.py:593: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
            "  logger.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 864x576 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "!pip install gym\n",
        "!pip install gym[classic_control]\n",
        "\n",
        "import gym\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "env_cart = gym.make(\"CartPole-v0\", render_mode=\"rgb_array\", new_step_api=True)\n",
        "env_mount = gym.make(\"MountainCar-v0\", render_mode=\"rgb_array\", new_step_api=True)\n",
        "\n",
        "#plt.plot(env_cart)\n",
        "#plt.plot(env_mount)\n",
        "\n",
        "fig = plt.figure(figsize=(12,8))\n",
        "fig, ax_lst = plt.subplots(1, 2)\n",
        "\n",
        "env_cart.reset()\n",
        "env_mount.reset()\n",
        "\n",
        "ax_lst[0].imshow(env_cart.render()[0])\n",
        "ax_lst[1].imshow(env_mount.render()[0])\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fca3781a-229f-451a-af3b-d776dff5dbdc",
      "metadata": {
        "id": "fca3781a-229f-451a-af3b-d776dff5dbdc"
      },
      "source": [
        "*L'environement CartPole évolue uniquement sur 1 dimension (une ligne) tandis que l'environement MountainCar évolue sur une fonction polynomiale (une courbe)*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0210ddeb-4cff-41e4-8bcf-70f45a6c9ad6",
      "metadata": {
        "id": "0210ddeb-4cff-41e4-8bcf-70f45a6c9ad6"
      },
      "source": [
        "# Implémentation du DQN"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e0070fc-e838-4592-8db2-7b941d75c92e",
      "metadata": {
        "id": "7e0070fc-e838-4592-8db2-7b941d75c92e"
      },
      "source": [
        "L'architecture neuronale pour résoudre le Deep Q-Learning (DQN) est un réseau de neurones profond. Il est composé d'une couche d'entrée, d'une couche cachée et d'une couche de sortie. La couche d'entrée prend en entrée les données de l'environnement (position et vitesse de la voiture) et les convertit en un vecteur d'entrée. La couche cachée est composée de plusieurs couches de neurones qui traitent les données et les convertissent en un vecteur de sortie. La couche de sortie prend en entrée le vecteur de sortie de la couche cachée et produit une action à effectuer (accélérer, freiner ou ne rien faire).\n",
        "\n",
        "**Q4. Implémentez ce réseau.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "7a508645-ab27-46a9-8689-4b1235d3ef6a",
      "metadata": {
        "id": "7a508645-ab27-46a9-8689-4b1235d3ef6a"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Model(nn.Module):\n",
        "  def __init__(self, obs_shape, num_actions):\n",
        "    super(Model, self).__init__()\n",
        "    self.obs_shape = obs_shape\n",
        "    self.num_actions = num_actions\n",
        "    self.net = torch.nn.Sequential(\n",
        "        torch.nn.Linear(self.obs_shape, 256), #2 first are position and speed\n",
        "        torch.nn.ReLU(),\n",
        "        torch.nn.Linear(256, self.num_actions)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.net(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a741d29d-518d-4932-b39d-506db2bc678f",
      "metadata": {
        "id": "a741d29d-518d-4932-b39d-506db2bc678f"
      },
      "source": [
        "La fonction de perte est une fonction qui mesure la différence entre les valeurs prédites et les valeurs réelles. \n",
        "Dans le cas du DQN, la fonction de perte mesure la différence entre la valeur prédite par le réseau de neurones et la valeur réelle obtenue par l'environnement en utilisant la formule suivante :\n",
        "\n",
        "La fonction de perte est donnée par :\n",
        "$$L(\\theta) = \\mathbb{E}_{s_t,a_t \\sim \\rho(.)}[(Q_{\\theta}(s_t,a_t) - y_t)^2],$$\n",
        "où $\\theta$ est le vecteur de paramètres du réseau de neurones, $\\rho$ est la distribution de probabilité de l'état et de l'action, $Q_{\\theta}$ est la fonction de valeur estimée par le réseau de neurones, et $y_t$ est la cible de la fonction de valeur.\n",
        "\n",
        "$$y_t = \\mathbb{E}_{s_{t+1},a_{t+1} \\sim \\rho(.)}[r(s_t,a_t) + \\gamma \\max_{a}Q_{\\theta}(s_{t+1},a_{t+1})].$$\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_loss(\n",
        "    model: nn.Module,\n",
        "    state: torch.Tensor, \n",
        "    next_state: torch.Tensor,\n",
        "    reward: torch.Tensor,\n",
        "    action: torch.Tensor,\n",
        "    done: torch.Tensor,\n",
        "    #model_next: torch.Tensor\n",
        "):\n",
        "    \"\"\"\n",
        "    Compute the DQN agent loss\n",
        "    \"\"\"\n",
        "    # A compléter\n",
        "\n",
        "    num_actions = len(action)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      qvals_next = model(next_state).max(-1)[0]\n",
        "\n",
        "    yt = (reward + qvals_next).mean()\n",
        "\n",
        "    qvals = model(state)\n",
        "    #one_hot_actions = F.one_hot(torch.LongTensor(action.cpu()), num_actions).cuda()\n",
        "\n",
        "    loss = ((qvals - yt)**2).mean() #(reward + qvals_next - torch.sum(qvals*one_hot_actions, -1)).mean()\n",
        "\n",
        "    loss.backward()\n",
        "    #model.optimizer.step()\n",
        "\n",
        "    return loss"
      ],
      "metadata": {
        "id": "471OoXZrSgzT"
      },
      "id": "471OoXZrSgzT",
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "e4020bf6-eb8c-4411-9eed-07fc69ca6190",
      "metadata": {
        "id": "e4020bf6-eb8c-4411-9eed-07fc69ca6190"
      },
      "source": [
        "**Q6. Complétez le code suivant et tracez la courbe de la récompense.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "id": "97e6f81b-dc4b-4385-bcc2-39297b72901c",
      "metadata": {
        "id": "97e6f81b-dc4b-4385-bcc2-39297b72901c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e70ca13-16e6-458e-ca36-1e974d0ff899"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:47<00:00,  2.10it/s, reward=-200]\n"
          ]
        }
      ],
      "source": [
        "import typing as t\n",
        "import gym\n",
        "import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from collections import deque\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# DQN Agent\n",
        "class DQNAgent:\n",
        "    def __init__(self, env, device):\n",
        "        self.env = env\n",
        "        self.device = device\n",
        "        self.state_size = env.observation_space.shape[0]\n",
        "        self.action_size = env.action_space.n\n",
        "        self._memory: t.Deque[t.Tuple[np.ndarray, int, float, np.ndarray, bool]] = deque(maxlen=2000)\n",
        "        self.batch_size = 32\n",
        "        self.gamma = 0.95    # discount rate\n",
        "        self.epsilon = 1.0  # exploration rate\n",
        "        self.epsilon_min = 0.01\n",
        "        self.epsilon_decay = 0.995\n",
        "        self.learning_rate = 0.001\n",
        "        self.model = self._build_model().to(self.device)\n",
        "        #self.m = self._build_model().to(self.device)\n",
        "\n",
        "    def _build_model(self):\n",
        "        # Neural Net for Deep-Q learning Model\n",
        "        # Ajoutez votre code ici\n",
        "        model = Model(2, self.action_size)\n",
        "\n",
        "        return model\n",
        "\n",
        "    def remember(self, state: np.ndarray, action: int, reward: float, next_state: np.ndarray, done: bool):\n",
        "        if (isinstance(state, np.ndarray) and state.shape[0] == 2):\n",
        "          self._memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def act(self, state: np.ndarray) -> int:\n",
        "        # Exploration : choisir une action aléatoire avec une probabilité epsilon\n",
        "        # Ajoutez votre code ici\n",
        "        if (random.uniform(0, 1) < self.epsilon) or not(isinstance(state, np.ndarray) and state.shape[0] == 2):\n",
        "            return self.env.action_space.sample()\n",
        "\n",
        "\n",
        "        # Choisir une action à partir du modèle\n",
        "        # Ajoutez votre code ici\n",
        "\n",
        "        return int(torch.argmax(self.model(torch.from_numpy(state).to(self.device))).item())\n",
        "\n",
        "    def replay(self):\n",
        "        minibatch = random.sample(self._memory, self.batch_size)\n",
        "\n",
        "        #print(minibatch)\n",
        "\n",
        "        states = torch.from_numpy(np.vstack([x[0] for x in minibatch])).float().to(self.device)\n",
        "        actions = torch.from_numpy(np.vstack([x[1] for x in minibatch])).long().to(self.device)\n",
        "        rewards = torch.from_numpy(np.vstack([x[2] for x in minibatch])).float().to(self.device)\n",
        "        next_states = torch.from_numpy(np.vstack([x[3] for x in minibatch])).float().to(self.device)\n",
        "        dones = torch.from_numpy(np.vstack([x[4] for x in minibatch])).float().to(self.device)\n",
        "\n",
        "        q_values = self.model(states)\n",
        "        q_next_state = self.model(next_states)\n",
        "        self.optimizer.zero_grad()\n",
        "\n",
        "        # Optimisation du réseau de neurones\n",
        "        # Ajoutez votre code ici\n",
        "        loss = compute_loss(self.model, states, next_states, rewards, actions, dones)\n",
        "        #loss.backwards()\n",
        "\n",
        "        self.optimizer.step()\n",
        "\n",
        "        # Réduction d'epsilon\n",
        "        # Ajoutez votre code ici\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "    def train(self, episodes: int):\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
        "        scores = []\n",
        "        with tqdm.tqdm(range(episodes)) as t:\n",
        "            for e in t:\n",
        "                state, _ = self.env.reset()\n",
        "                done = False\n",
        "                score = 0\n",
        "                while not done:\n",
        "                    action = self.act(state)\n",
        "                    #action = action.detach().numpy()\n",
        "                    next_state, reward, done, _, = self.env.step(action)\n",
        "                    self.remember(state, action, reward, next_state, done)\n",
        "                    state = next_state\n",
        "                    score += reward\n",
        "                    if len(self._memory) > self.batch_size:\n",
        "                        self.replay()\n",
        "                scores.append(score)\n",
        "                t.set_postfix(reward=score)\n",
        "        return scores\n",
        "    \n",
        "\n",
        "# Initialize environment\n",
        "env = gym.make('MountainCar-v0')\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Initialize agent\n",
        "agent = DQNAgent(env, device)\n",
        "\n",
        "# Train agent\n",
        "scores = agent.train(episodes=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4206ea24-a9dd-4994-b295-64d26967d74c",
      "metadata": {
        "id": "4206ea24-a9dd-4994-b295-64d26967d74c"
      },
      "source": [
        "**Q7. Visualisez la vidéo d'un cas d'échec et d'un cas de réussite.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "id": "8b7ad1e1-57c0-4acc-8f6d-12f605ab5a10",
      "metadata": {
        "id": "8b7ad1e1-57c0-4acc-8f6d-12f605ab5a10"
      },
      "outputs": [],
      "source": [
        "# Ajoutez votre code ici"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6744175a-7a26-4181-8656-9c3d83216d91",
      "metadata": {
        "id": "6744175a-7a26-4181-8656-9c3d83216d91"
      },
      "source": [
        "*[Ajoutez votre réponse ici]*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "834f749c-3310-4cb5-93bc-2e2604006fab",
      "metadata": {
        "id": "834f749c-3310-4cb5-93bc-2e2604006fab"
      },
      "source": [
        "# Double DQN"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "897bb918-1578-458e-a0d4-922666eb1a06",
      "metadata": {
        "id": "897bb918-1578-458e-a0d4-922666eb1a06"
      },
      "source": [
        "\n",
        "Le Double DQN est un algorithme d'apprentissage par renforcement qui combine le DQN avec une technique appelée Prise de décision double. Le Double DQN permet à l'agent d'explorer plus intelligemment des environnements complexes et imprévisibles. L'algorithme est conçu pour maximiser la valeur estimée sur la base des informations disponibles.\n",
        "\n",
        "L'algorithme commence par initialiser des paramètres pour le modèle de réseau neuronal profond et deux cibles de réseaux distincts, X et Y. Les deux cibles sont des copies parallèles, mais le modèle X est mis à jour plus fréquemment que le modèle Y. Quand un agent prend une action, il utilise le réseau X pour calculer la valeur estimée et recueille le feedback après l'action. L'agent utilise ensuite le réseau Y pour déterminer quelle était l'action optimale à partir de cette valeur estimée. \n",
        "\n",
        "Cette procédure est appelée *prise de décision double*. Elle réduit l'instabilité et les biais de l'exploration liés à la maximisation prématurée.\n",
        "\n",
        "**Q8. Ajoutez le Double DQN sur votre implémentation précédente.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "id": "6ad48e09-aade-4518-86e6-48cb2bdd5b83",
      "metadata": {
        "id": "6ad48e09-aade-4518-86e6-48cb2bdd5b83",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62d071e6-de75-4950-c1b0-b39aa4311584"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.8/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "100%|██████████| 1000/1000 [10:16<00:00,  1.62it/s]\n"
          ]
        }
      ],
      "source": [
        "import typing as t\n",
        "import gym\n",
        "import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from collections import deque\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "def compute_loss2(\n",
        "    model: nn.Module,\n",
        "    state: torch.Tensor, \n",
        "    next_state: torch.Tensor,\n",
        "    reward: torch.Tensor,\n",
        "    action: torch.Tensor,\n",
        "    done: torch.Tensor,\n",
        "    model_next: torch.Tensor\n",
        "):\n",
        "    \"\"\"\n",
        "    Compute the DQN agent loss\n",
        "    \"\"\"\n",
        "    # A compléter\n",
        "\n",
        "    num_actions = len(action)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      qvals_next = model_next(next_state).max(-1)[0]\n",
        "\n",
        "    yt = (reward + qvals_next).mean()\n",
        "\n",
        "    qvals = model(state)\n",
        "    #one_hot_actions = F.one_hot(torch.LongTensor(action.cpu()), num_actions).cuda()\n",
        "\n",
        "    loss = ((qvals - yt)**2).mean() #(reward + qvals_next - torch.sum(qvals*one_hot_actions, -1)).mean()\n",
        "\n",
        "    #loss.backward()\n",
        "    #model.optimizer.step()\n",
        "\n",
        "    return loss\n",
        "\n",
        "# DQN Agent\n",
        "class DDQNAgent:\n",
        "    def __init__(self, env, device):\n",
        "        self.env = env\n",
        "        self.device = device\n",
        "        self.state_size = env.observation_space.shape[0]\n",
        "        self.action_size = env.action_space.n\n",
        "        self._memory: t.Deque[t.Tuple[np.ndarray, int, float, np.ndarray, bool]] = deque(maxlen=2000)\n",
        "        self.batch_size = 32\n",
        "        self.gamma = 0.95    # discount rate\n",
        "        self.epsilon = 1.0  # exploration rate\n",
        "        self.epsilon_min = 0.01\n",
        "        self.epsilon_decay = 0.995\n",
        "        self.learning_rate = 0.001\n",
        "        self.model = self._build_model().to(self.device)\n",
        "        self.target_model = self._build_model().to(self.device)\n",
        "\n",
        "    def _build_model(self):\n",
        "        # Neural Net for Deep-Q learning Model\n",
        "        # Ajoutez votre code ici\n",
        "        model = Model(2, self.action_size)\n",
        "\n",
        "        return model\n",
        "\n",
        "    def remember(self, state: np.ndarray, action: int, reward: float, next_state: np.ndarray, done: bool):\n",
        "        if (isinstance(state, np.ndarray) and state.shape[0] == 2):\n",
        "          self._memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def act(self, state: np.ndarray) -> int:\n",
        "        # Exploration : choisir une action aléatoire avec une probabilité epsilon\n",
        "        # Ajoutez votre code ici\n",
        "        if (random.uniform(0, 1) < self.epsilon) or not(isinstance(state, np.ndarray) and state.shape[0] == 2):\n",
        "            return self.env.action_space.sample()\n",
        "\n",
        "\n",
        "        # Choisir une action à partir du modèle\n",
        "        # Ajoutez votre code ici\n",
        "\n",
        "        return int(torch.argmax(self.model(torch.from_numpy(state).to(self.device))).item())\n",
        "\n",
        "    def replay(self):\n",
        "        minibatch = random.sample(self._memory, self.batch_size)\n",
        "\n",
        "        #print(minibatch)\n",
        "\n",
        "        states = torch.from_numpy(np.vstack([x[0] for x in minibatch])).float().to(self.device)\n",
        "        actions = torch.from_numpy(np.vstack([x[1] for x in minibatch])).long().to(self.device)\n",
        "        rewards = torch.from_numpy(np.vstack([x[2] for x in minibatch])).float().to(self.device)\n",
        "        next_states = torch.from_numpy(np.vstack([x[3] for x in minibatch])).float().to(self.device)\n",
        "        dones = torch.from_numpy(np.vstack([x[4] for x in minibatch])).float().to(self.device)\n",
        "\n",
        "        q_values = self.model(states)\n",
        "        q_next_state = self.model(next_states)\n",
        "        self.optimizer.zero_grad()\n",
        "        self.optimizer2.zero_grad()\n",
        "\n",
        "        # Optimisation du réseau de neurones\n",
        "        # Ajoutez votre code ici\n",
        "        loss = compute_loss(self.target_model, states, next_states, rewards, actions, dones)\n",
        "\n",
        "        loss2 = compute_loss2(self.model, states, next_states, rewards, actions, dones, self.target_model)\n",
        "\n",
        "        if (random.uniform(0, 1) < 0.5):\n",
        "            loss2.backward()\n",
        "            self.optimizer2.step()\n",
        "\n",
        "        self.optimizer.step()\n",
        "\n",
        "        # Réduction d'epsilon\n",
        "        # Ajoutez votre code ici\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "    def train(self, episodes):\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
        "        self.optimizer2 = optim.Adam(self.target_model.parameters(), lr=self.learning_rate)\n",
        "        scores = []\n",
        "        for e in tqdm.tqdm(range(episodes)):\n",
        "            state, _ = self.env.reset()\n",
        "            done = False\n",
        "            score = 0\n",
        "            while not done:\n",
        "                action = self.act(state)\n",
        "                next_state, reward, done, _= self.env.step(action)\n",
        "                self.remember(state, action, reward, next_state, done)\n",
        "                state = next_state\n",
        "                score += reward\n",
        "                if len(self._memory) > self.batch_size:\n",
        "                    self.replay()\n",
        "            scores.append(score)\n",
        "            if e % 10 == 0:\n",
        "                self.target_model.load_state_dict(self.model.state_dict())\n",
        "        return scores\n",
        "    \n",
        "\n",
        "# Initialize environment\n",
        "env = gym.make('MountainCar-v0')\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Initialize agent\n",
        "agent = DDQNAgent(env, device)\n",
        "\n",
        "# Train agent\n",
        "scores = agent.train(episodes=1000)\n",
        "# Solution fin"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3697184-1ab4-4c3d-8e6c-b804575e1c4f",
      "metadata": {
        "id": "c3697184-1ab4-4c3d-8e6c-b804575e1c4f"
      },
      "source": [
        "*[Ajoutez votre réponse ici]*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62e4cd22-bd90-4910-9941-382230df8067",
      "metadata": {
        "id": "62e4cd22-bd90-4910-9941-382230df8067"
      },
      "source": [
        "**Q9. En cherchant sur Internet, proposez une série d'améliorations possibles et décrivez les**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e069d30-a4d2-4b01-addc-5677945c8854",
      "metadata": {
        "id": "8e069d30-a4d2-4b01-addc-5677945c8854"
      },
      "source": [
        "*[Ajoutez votre réponse ici]*"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0rc1"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}